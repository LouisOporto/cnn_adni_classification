{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported Libraries for CNN Project to Classify Alzhiemers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include needed import modules\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for loading ADNI data as Nifti file and loading ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class for loading NiftiDataset\n",
    "class NiftiDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.images = self.get_all_nifti_files(image_dir)\n",
    "\n",
    "    def get_all_nifti_files(self, folder_path):\n",
    "        # Recusrievly find files in folder_path that match regex\n",
    "        nii_files = []\n",
    "        for subdir, _, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".nii\"):\n",
    "                    nii_files.append(os.path.join(subdir, file))\n",
    "        print(f\"Loaded {len(nii_files)} .nii files from {folder_path}\\n\")\n",
    "        return nii_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.images[idx]).get_fdata()  # Load NIfTI image as NumPy array\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        \n",
    "        \n",
    "        # Normalize the image intensity\n",
    "        img = (img - np.min(img)) / (np.max(img) - np.min(img))  # Min-max normalization\n",
    "        \n",
    "        # Convert to 3D single-channel format (ResNet expects 3-channel input)\n",
    "        img = np.stack(img * 3, axis=-1)  # Convert grayscale to RGB\n",
    "        img = np.transpose(img, (2, 1, 0)) # Change to (C, H, W)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(Image.fromarray(img.astype('uint8'), 'RGB'))\n",
    "\n",
    "        label = self.get_label_from_filename(self.images[idx])  # Implement a function to map filenames to labels\n",
    "        return img, label\n",
    "\n",
    "    def get_label_from_filename(self, filename):\n",
    "        \"\"\"Extract label based on filename convention.\"\"\"\n",
    "        if \"CN\" in filename:\n",
    "            return 0  # Cognitively Normal\n",
    "        elif \"MCI\" in filename:\n",
    "            return 1  # Mild Cognitive Impairment\n",
    "        elif \"AD\" in filename:\n",
    "            return 2  # Alzheimer's Disease\n",
    "        return -1  # Unknown label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class for loading ResNet-40 model\n",
    "class AlzheimerResNet(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(AlzheimerResNet, self).__init__()\n",
    "        self.model = models.resnet50(weights=\"ResNet50_Weights.DEFAULT\")\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_ftrs, num_classes) # Modify the output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading ADNI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 38 .nii files from .\\adni_dataset\\ADNI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create transformations and loading dataset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 244)), # Resize to fit ResNet-50\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]) # Normalizing single-channel data\n",
    "])\n",
    "\n",
    "\n",
    "full_dataset = NiftiDataset(image_dir=\".\\\\adni_dataset\\\\ADNI\", transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 244])\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x28daf246710>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Sample some of the images\n",
    "adni_sample, label = train_dataset[0]\n",
    "print(adni_sample.shape)\n",
    "print(label) # 0 = CN, 1 = MCI, 2 = AD (Current sample is AD)\n",
    "plt.imshow(adni_sample[:,:,80]) # Image is contained in a 256 x 256 image with 166 slices (MRI images)\n",
    "# ax = plt.axes(projection='3d')\n",
    "# ax.plot(test[0][0], test[1], 'green')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pretrained Resnet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Resnet-50 model\n",
    "model = AlzheimerResNet().to(device)\n",
    "\n",
    "# Defline loss function\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalulate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criteria, optimizer, train_loader, val_loader, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criteria(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        val_acc = evalulate_model(model, val_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.0650, Train Acc: 100.00%, Val Acc: 100.00%\n",
      "Epoch [2/10], Loss: 0.9068, Train Acc: 100.00%, Val Acc: 100.00%\n",
      "Epoch [3/10], Loss: 0.7227, Train Acc: 100.00%, Val Acc: 100.00%\n",
      "Epoch [4/10], Loss: 0.5068, Train Acc: 100.00%, Val Acc: 100.00%\n",
      "Epoch [5/10], Loss: 0.2412, Train Acc: 100.00%, Val Acc: 100.00%\n",
      "Epoch [6/10], Loss: 0.1139, Train Acc: 100.00%, Val Acc: 100.00%\n",
      "Epoch [7/10], Loss: 0.0551, Train Acc: 100.00%, Val Acc: 100.00%\n",
      "Epoch [8/10], Loss: 0.0295, Train Acc: 100.00%, Val Acc: 100.00%\n",
      "Epoch [9/10], Loss: 0.0170, Train Acc: 100.00%, Val Acc: 100.00%\n",
      "Epoch [10/10], Loss: 0.0108, Train Acc: 100.00%, Val Acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "train_model(model, criteria, optimizer, train_loader, val_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"alzheimer_resnet50.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
