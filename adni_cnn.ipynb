{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported Libraries for CNN Project to Classify Alzhiemers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Include needed import modules\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for loading ADNI data as Nifti file and loading ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create class for loading NiftiDataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mNiftiDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_dir, transform=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m      4\u001b[39m         \u001b[38;5;28mself\u001b[39m.image_dir = image_dir\n",
      "\u001b[31mNameError\u001b[39m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Create class for loading NiftiDataset\n",
    "class NiftiDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.images = self.get_all_nifti_files(image_dir)\n",
    "\n",
    "    def get_all_nifti_files(self, folder_path):\n",
    "        # Recusrievly find files in folder_path that match regex\n",
    "        nii_files = []\n",
    "        for subdir, _, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".nii\"):\n",
    "                    nii_files.append(os.path.join(subdir, file))\n",
    "        print(f\"Loaded {len(nii_files)} .nii files from {folder_path}\\n\")\n",
    "        return nii_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.images[idx]).get_fdata()  # Load NIfTI image as NumPy array\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        \n",
    "        \n",
    "        # Normalize the image intensity\n",
    "        img = (img - np.min(img)) / (np.max(img) - np.min(img))  # Min-max normalization\n",
    "        \n",
    "        # Convert to 3D single-channel format (ResNet expects 3-channel input)\n",
    "        img = np.stack(img * 3, axis=-1)  # Convert grayscale to RGB\n",
    "        img = np.transpose(img, (2, 1, 0)) # Change to (C, H, W)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(Image.fromarray(img.astype('uint8'), 'RGB'))\n",
    "\n",
    "        label = self.get_label_from_filename(self.images[idx])  # Implement a function to map filenames to labels\n",
    "        return img, label\n",
    "\n",
    "    def get_label_from_filename(self, filename):\n",
    "        \"\"\"Extract label based on filename convention.\"\"\"\n",
    "        if \"CN\" in filename:\n",
    "            return 0  # Cognitively Normal\n",
    "        elif \"MCI\" in filename:\n",
    "            return 1  # Mild Cognitive Impairment\n",
    "        elif \"AD\" in filename:\n",
    "            return 2  # Alzheimer's Disease\n",
    "        return -1  # Unknown label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class for loading ResNet-40 model\n",
    "class AlzheimerResNet(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(AlzheimerResNet, self).__init__()\n",
    "        self.model = models.resnet50(weights=\"ResNet50_Weights.DEFAULT\")\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_ftrs, num_classes) # Modify the output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading ADNI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2294 .nii files from .\\adni_dataset\\ADNI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create transformations and loading dataset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 244)), # Resize to fit ResNet-50\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]) # Normalizing single-channel data\n",
    "])\n",
    "\n",
    "\n",
    "full_dataset = NiftiDataset(image_dir=\".\\\\adni_dataset\\\\ADNI\", transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 244])\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f48acd8830>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Sample some of the images\n",
    "adni_sample, label = train_dataset[0]\n",
    "print(adni_sample.shape)\n",
    "print(label) # 0 = CN, 1 = MCI, 2 = AD (Current sample is AD)\n",
    "plt.imshow(adni_sample[:,:,80]) # Image is contained in a 256 x 256 image with 166 slices (MRI images)\n",
    "# ax = plt.axes(projection='3d')\n",
    "# ax.plot(test[0][0], test[1], 'green')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pretrained Resnet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Resnet-50 model\n",
    "model = AlzheimerResNet().to(device)\n",
    "\n",
    "# Defline loss function\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalulate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criteria, optimizer, train_loader, val_loader, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criteria(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        val_acc = evalulate_model(model, val_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4080\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(torch.cuda.get_device_name(\u001b[32m0\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriteria\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, criteria, optimizer, train_loader, val_loader, num_epochs)\u001b[39m\n\u001b[32m      5\u001b[39m correct = \u001b[32m0\u001b[39m\n\u001b[32m      6\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\louis\\Desktop\\adni_cnn\\cnn_adni_classification\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\louis\\Desktop\\adni_cnn\\cnn_adni_classification\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\louis\\Desktop\\adni_cnn\\cnn_adni_classification\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\louis\\Desktop\\adni_cnn\\cnn_adni_classification\\env\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mNiftiDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     23\u001b[39m img = np.array(img, dtype=np.float32)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Normalize the image intensity\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m img = (img - np.min(img)) / (\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m - np.min(img))  \u001b[38;5;66;03m# Min-max normalization\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Convert to 3D single-channel format (ResNet expects 3-channel input)\u001b[39;00m\n\u001b[32m     30\u001b[39m img = np.stack(img * \u001b[32m3\u001b[39m, axis=-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Convert grayscale to RGB\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\louis\\Desktop\\adni_cnn\\cnn_adni_classification\\env\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3199\u001b[39m, in \u001b[36mmax\u001b[39m\u001b[34m(a, axis, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   3080\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[32m   3081\u001b[39m \u001b[38;5;129m@set_module\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   3082\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmax\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=np._NoValue, initial=np._NoValue,\n\u001b[32m   3083\u001b[39m          where=np._NoValue):\n\u001b[32m   3084\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3085\u001b[39m \u001b[33;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[32m   3086\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3197\u001b[39m \u001b[33;03m    5\u001b[39;00m\n\u001b[32m   3198\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3199\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3200\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\louis\\Desktop\\adni_cnn\\cnn_adni_classification\\env\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:86\u001b[39m, in \u001b[36m_wrapreduction\u001b[39m\u001b[34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     84\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis=axis, out=out, **passkwargs)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_model(model, criteria, optimizer, train_loader, val_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"alzheimer_resnet50.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
